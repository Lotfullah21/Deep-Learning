{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Convolutional Neural Net.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNUxlxL4CYEMAm+aQFOWmVa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lotfullah21/Deep-Learning/blob/main/Convolutional_Neural_Net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Convolutinal Neural Network**\n",
        "\n",
        "Convolutional Networks are simly neural networks that use convolution in place of general matrix multiplication, in at least of their layers.\n",
        "this is also the family of neural networks, But here we are using another approach to create the models which are invarient to certain transformation of the input to build the invarience properties into the structure of a neural network.\n",
        "### The Convolution Operation:\n",
        "Convolution is an operation on two function of real valued argument\n",
        "\n",
        "![Screenshot 2022-07-14 234509.jpg](data:image/jpeg;base64,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).\n",
        "\n",
        "the convolution is defined for any function for which the above integral is defined. Here the X(t) and W(t) are the position of the object at time t and the kernel respectively.\n",
        "Kernel is a filter that extract meaningful features from an object.\n",
        "the output of the above operation sometimes referred as feature map."
      ],
      "metadata": {
        "id": "S0kTsuXOkSN1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convolution leverages three important ideas that help a machine learning system.\n",
        "1. Sparse interactions\n",
        "2. Parameter Sharing\n",
        "3. Equivarient Representations.\n",
        "\n",
        "## Why Convolutional Neural Net(CNN) ?\n",
        "Bsically in most of the Machine learning task we are looking for meaningful features or patterns, and based on that we can predict the future event or detect an object and many more applications, in image data we do not need the precise location of an object, for exmple to detect the on a human being face, just we need eyes, it does not matter in which part of the image it appears, in other word, the location of pattern is not important, the presence of pattern is important, But Conventional networks are sensitive to location, if we train a network to detect an eye, then it will look the exact location of that eye that appered in the image, while testing if the eyes are not present in the location where the Net trained on,it will misclassify it.\n",
        "\n",
        "if we want to that kind of network to perform well, then we need so much of training data to feed in.\n",
        "\n",
        "We need a network that must be shift invarient, which means that by small changes, it should not effect our learning process.\n"
      ],
      "metadata": {
        "id": "Dip87qrR9Orn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the effect which convolution is going to have on our image is some of the feature might be given more importance as a result of convolution and that depends on kernel or filter values, for example in an image we might be interested more in horizinatal lines, so we will choose our filter value such that it extract the horizintal features.\n"
      ],
      "metadata": {
        "id": "lCHAzkJxXI-m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advantage\n",
        "they can process inputs with varying spatial extents, these kinds of inputs simply cannot be represented by tradational matrix multiplication-based neural nets.\n",
        "\n",
        "it can be used to output a high_dimensional structured objects, rather than just predicting a class label."
      ],
      "metadata": {
        "id": "T1zT1BwEe_NR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use fashion_mnist dataset here as it is wideyl used and simple for understanding purpose"
      ],
      "metadata": {
        "id": "YSVm6BKmbRk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Load the data \n",
        "fmnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = fmnist.load_data()\n",
        "\n",
        "# Normalizing the pixel values\n",
        "training_images = training_images / 255.0\n",
        "test_images = test_images / 255.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUMG1nYMXtBV",
        "outputId": "802960f5-cc25-488f-a4b7-a7cf3b4d7a25"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Kp7KcWAYgrsc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pooling\n",
        "Definition: A pooling function replaces the output of the network at a certain location with a summary statistics of nearby outputs.\n",
        "this operation is done after the convolution operation, pooling+convolution makes our model much powerful in detecting and giving more importance to the detected features.\n",
        "pooling is done to modify the output further after convolution operation, it helps to make the representation approximately invarient to small translations.\n",
        "\n",
        "If an object in an image is at area A and through convolution a feature is detected at the output at area B, then the same feature would be detected when the object in the image is translated to A'.\n",
        "\n",
        "invariance to translation can be useful if we care more about the presesnce of a feature rahter than its location.\n",
        "\n",
        "Pooling also can be useful for handling the inputs of varying size, for example getting the images of different heights and widths or getting the speech of different length and it also it reduces the size of the parameters which can results in high speed and needing of less memory storage.\n",
        "\n",
        "Here we use MaxPooling which takes maximum value after convolution operation and ignore the rest\n",
        "\n",
        "When the task involves incorporating information from very distant locations in the input, then pooling might not be a good option, because the important features are far from each other we might ignore or loose those features during pooling."
      ],
      "metadata": {
        "id": "EwGwo8Y3b3NG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "                                                         \n",
        "  # Adding convolutions \n",
        "  # 32 filters with each one 3 in heght and 3 in width\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  # Adding MaxPooling of size 2x2\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "\n",
        "  # Adding Dense Layers and Flatten layer\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  \n",
        "  # the output layer with 10 neurons each one represents a class.\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "NUT0V94Kg21y"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are Flattening our data,becuse the position doesn not matter here, just we need the presence of the information."
      ],
      "metadata": {
        "id": "C3MQaug6hMfD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Summary\n",
        "we can have our model summary by summary method to observe what happened in our network during adding the hidden layers"
      ],
      "metadata": {
        "id": "LainRpPljHcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaPnKLeKjEem",
        "outputId": "26b864bf-2fcd-4a1a-fb7e-40107bd161d1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 11, 11, 32)        9248      \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 5, 5, 32)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 3, 3, 32)          9248      \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 1, 1, 32)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 32)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               4224      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 24,330\n",
            "Trainable params: 24,330\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Choosing the Hyperparameters and traing the model"
      ],
      "metadata": {
        "id": "ae6V0zcDjk6p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here also we use the Gradient descent algorithm, from that family the 'adam' one and the loss as ours categorical, we use categorical loss.\n",
        "\n",
        "epochs means how many time we are going through entire data set, here we choose 10, means 10 time through training data to compute the losses and adjust the learnable parameters"
      ],
      "metadata": {
        "id": "JPRMiQFwiLhe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Training the model\n",
        "model.fit(training_images, training_labels, epochs=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqyCWZytbgon",
        "outputId": "3bbc9b5e-75f4-4132-9fcf-f042e5ea8aa4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 17s 3ms/step - loss: 0.6592 - accuracy: 0.7553\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4495 - accuracy: 0.8349\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3885 - accuracy: 0.8568\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3530 - accuracy: 0.8691\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3263 - accuracy: 0.8789\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3057 - accuracy: 0.8867\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2891 - accuracy: 0.8920\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2785 - accuracy: 0.8966\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2657 - accuracy: 0.9008\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2557 - accuracy: 0.9041\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6f002f6b90>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating the model on the test set"
      ],
      "metadata": {
        "id": "FOr6Rt10jeE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss = model.evaluate(test_images, test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bOQbIY8jAng",
        "outputId": "26a5be60-e5df-4c9e-8a5a-162f900abda4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.3460 - accuracy: 0.8748\n"
          ]
        }
      ]
    }
  ]
}