{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Activation Functions.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNZZoh1etCd9Q4Is9vu7DqM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lotfullah21/Deep-Learning/blob/main/Activation_Functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VV7Gj7FURTVJ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Softmax Function\n",
        "In both softmax regression and neural networks with Softmax outputs, N outputs are generated and one output is selected as the predicted category. In both cases a vector $\\mathbf{z}$ is generated by a linear function which is applied to a softmax function. The softmax function converts $\\mathbf{z}$  into a probability distribution as described below. After applying softmax, each output will be between 0 and 1 and the outputs will add to 1, so that they can be interpreted as probabilities. The larger inputs  will correspond to larger output probabilities.\n"
      ],
      "metadata": {
        "id": "zfTMlhyUvpk6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The softmax function can be written:\n",
        "$$a_j = \\frac{e^{z_j}}{ \\sum_{k=1}^{N}{e^{z_k} }} \\tag{1}$$\n",
        "The output $\\mathbf{a}$ is a vector of length N, so for softmax regression, you could also write:\n",
        "\\begin{align}\n",
        "\\mathbf{a}(x) =\n",
        "\\begin{bmatrix}\n",
        "P(y = 1 | \\mathbf{x}; \\mathbf{w},b) \\\\\n",
        "\\vdots \\\\\n",
        "P(y = N | \\mathbf{x}; \\mathbf{w},b)\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\frac{1}{ \\sum_{k=1}^{N}{e^{z_k} }}\n",
        "\\begin{bmatrix}\n",
        "e^{z_1} \\\\\n",
        "\\vdots \\\\\n",
        "e^{z_{N}} \\\\\n",
        "\\end{bmatrix} \\tag{2}\n",
        "\\end{align}\n"
      ],
      "metadata": {
        "id": "awbAblrHxTve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from IPython.display import display,Markdown, Latex\n",
        "from sklearn.datasets import make_blobs\n",
        "from matplotlib.widgets import Slider"
      ],
      "metadata": {
        "id": "5rt8NQ34vq-G"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax_fn(x):\n",
        "  ez = np.exp(x)\n",
        "  az = ez/ np.sum(ez)\n",
        "  return ez"
      ],
      "metadata": {
        "id": "0_rTwwYsv3tW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lets make a dataset"
      ],
      "metadata": {
        "id": "WmqIvZs8zipn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oGoRfs6wxkzR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "centers = [[-3,2],[4,1],[9,4],[5,3]]\n",
        "X_train, y_train = make_blobs(n_samples = 20000, centers = centers, cluster_std = 1.0, random_state = 20)"
      ],
      "metadata": {
        "id": "dn_7AWNHybip"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Dense(45, activation = \"relu\"),\n",
        "    Dense(30, activation = \"relu\"),\n",
        "    Dense(10, activation = \"relu\"),\n",
        "    Dense(4, activation = \"softmax\")\n",
        "])\n",
        "\n",
        "model.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              optimizer = tf.keras.optimizers.Adam(0.001)\n",
        "              )\n",
        "\n",
        "model.fit(X_train , y_train, epochs = 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIYb6fbuz9fG",
        "outputId": "57b34e83-73df-4976-b567-6fe7ecf8ccc5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 2s 2ms/step - loss: 0.5175\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.1945\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.1878\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.1837\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.1850\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.1848\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.1829\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.1837\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.1828\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.1820\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f47a1292690>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "here , the lastlayer (output) is a vector of probabilities"
      ],
      "metadata": {
        "id": "PCf6xUml1ZG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = model.predict(X_train)\n",
        "print(\"the prediction for first output is\", prediction[0])\n",
        "print(\"the largest value\", np.max(prediction[0]), \"the smallest value\",np.min(prediction[0]))\n",
        "print(\"the ouput with largest probablility is\", np.max(prediction[0]))\n",
        "print(\"the largest value\", np.max(prediction), \"the smallest value\",np.min(prediction))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAlexZ40069R",
        "outputId": "9d0c3abe-e977-4341-cd28-6d5517b857c2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the prediction for first output is [9.737790e-06 9.312184e-01 6.402269e-07 6.877116e-02]\n",
            "the largest value 0.9312184 the smallest value 6.402269e-07\n",
            "the ouput with largest probablility is 0.9312184\n",
            "the largest value 1.0 the smallest value 4.0749258e-25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prefered modlel \n",
        "when dealing with small values and especially if we put them into the exponential form , the round of matters.\n",
        "\n",
        "for example , X1 = 2/2000 = 0.00020000\n",
        "and X = (1 + 1/1000)  - (1 - 1/1000)  = 0.00019999978 , if we sum the fraction that should lead us to X1 , but the values changes and the 2nd is more accurate numerically, if we calculate them, \n",
        "\n",
        "Hence , the 2nd way is preferred way of doing so, and in the loss function we will put the fraction rather than pre computed value.\n",
        "the way to implement this in tensorflow, we will use linear instead of softmax in the output layer, and directly feed the z's : \n",
        "**( z = w * x = b )** to the model.\n",
        "for example we had  **a = softmax(z)**.\n",
        "\n",
        "we add from_logits=True in the loss function."
      ],
      "metadata": {
        "id": "q2JCW2Tq22Iu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preferred_model = Sequential(\n",
        "    [ \n",
        "        Dense(25, activation = 'relu'),\n",
        "        Dense(15, activation = 'relu'),\n",
        "        Dense(4, activation = 'linear')   \n",
        "    ]\n",
        ")\n",
        "preferred_model.compile(\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),  \n",
        "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
        ")\n",
        "\n",
        "preferred_model.fit(\n",
        "    X_train,y_train,\n",
        "    epochs=10\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXrvarep1sK7",
        "outputId": "eed789a8-a909-4930-f0a0-bf9a19c49e87"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 2s 2ms/step - loss: 0.5760\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.2329\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.1925\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.1857\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.1828\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.1821\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.1814\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.1810\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.1811\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.1802\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f47a10d2790>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Output\n",
        "Now , we will see the difference in the output, from the previous model, here the ouptuts are not in probability range, the are from positive to negaetive values."
      ],
      "metadata": {
        "id": "JFDHi5VK5cAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p_preferred = preferred_model.predict(X_train)\n",
        "print(f\"two example output vectors:\\n {p_preferred[:1]}\")\n",
        "print(\"largest value\", np.max(p_preferred), \"small|est value\", np.min(p_preferred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rka1Gpj04ckv",
        "outputId": "10edcc50-fbe0-4c8a-83b8-0f38b05650ba"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "two example output vectors:\n",
            " [[-4.7439165  5.769028  -6.7612715  3.2307675]]\n",
            "largest value 23.906492 small|est value -53.579815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "if we need the output to be in probability format, we need to feed them through a softmax function."
      ],
      "metadata": {
        "id": "yqZ5UHb45-7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sm_preferred = tf.nn.softmax(p_preferred).numpy()\n",
        "print(f\"two example output vectors:\\n {sm_preferred[:2]}\")\n",
        "print(\"largest value\", np.max(sm_preferred), \"smallest value\", np.min(sm_preferred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iku90xGE5mDx",
        "outputId": "d0708400-80bf-478c-aaa8-9785d2a6d5af"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "two example output vectors:\n",
            " [[2.5191302e-05 9.2675447e-01 3.3506146e-06 7.3217027e-02]\n",
            " [9.9999881e-01 7.3573369e-07 2.7264979e-19 4.7177386e-07]]\n",
            "largest value 1.0 smallest value 2.4570135e-34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "if we don't want to use softmax , we can loop through and using np.argmax(), find the largest values and assign them "
      ],
      "metadata": {
        "id": "j6DEMUeP6ZIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "    print( f\"{p_preferred[i]}, category: {np.argmax(p_preferred[i])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0D12RLSn6OaR",
        "outputId": "4d894066-73e4-4046-fd9f-22d50e162598"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-4.7439165  5.769028  -6.7612715  3.2307675], category: 1\n",
            "[ 11.165252   -2.9571445 -31.580847   -3.4015129], category: 0\n",
            "[-4.509475   4.421056  -9.387452   3.2539837], category: 1\n",
            "[ 14.087609  -5.33493  -33.198704  -5.889166], category: 0\n",
            "[ -2.8592422   6.43744   -12.837223    3.1675327], category: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SparseCategorialCrossentropy or CategoricalCrossEntropy\n",
        "Tensorflow has two potential formats for target values and the selection of the loss defines which is expected.\n",
        "- SparseCategorialCrossentropy: expects the target to be an integer corresponding to the index. For example, if there are 10 potential target values, y would be between 0 and 9. \n",
        "- CategoricalCrossEntropy: Expects the target value of an example to be one-hot encoded where the value at the target index is 1 while the other N-1 entries are zero. An example with 10 potential target values, where the target is 2 would be [0,0,1,0,0,0,0,0,0,0].\n"
      ],
      "metadata": {
        "id": "5G2Q0ZfR6vka"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source : https://www.coursera.org/learn/advanced-learning-algorithms/ungradedLab/7oaSL/softmax/lab?path=%2Fnotebooks%2FC2_W2_SoftMax.ipynb"
      ],
      "metadata": {
        "id": "pMfM-H_B6zT4"
      }
    }
  ]
}